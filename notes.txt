11/14 (oops did in reverse):
 - shifting towards more computation vs expertise as technology improves
 - learn architectures, not design
  | used reinforcement learning to control a 'controller' that would generate strings that described a neural net that would then be trained
   + searching, but with a more intelligent controller involved
   + what was the benefit vs just doing a dumb search? both methods generalizable, the controller was said to be optimized for the specific CIFAR dataset as well
   + answered that grid search would've been much slower because space much much larger
 - if you could take any dataset, train a few proxy neural nets that would generate one another, to produce final neural net that would fit to data, that would be ncie
 - use "cell" (basically a controlled search space) to generate net, made it more efficient
 - found that one of their generated nets performed slower on mobile phones because hardware different, so mobile phone latency taken into account in the reward that controlled the controller
 - also these nets transferred better to other datasets

 - searching for algorithms/optimizers
 - same controller, use the string method so reward given to string
 - optimizers, activation functions
  | x * sigmoid(x) was one of the best functions
    + i'ma use this lol
 - data processing has lots of potential because they're manually tuned (YES)
  | they did for augmentation
 
 - weakness for this is the "cell" needs a search space still that we need to define
 - search is still really expensive
 - ENAS (efficient neural architecture search) is one proposed solution, search space is one large graph and each node is an architecture
  | trick is the weights are shared between the nodes to speed up convergence

8/22:
 - decisions -><- observations, rewards
 - deep models allow solve reinforcement learning algos from end to end
 - interesting car jam situation with autonomous car
 - basic reinforcement learning = maximizing rewards but not enough to generalize to situations in real world
  | learn from demonstrations
  | observing the world
  | learn from other tasks (transfer learning), inferring intentions
 - predicting what actions will do, or planning actions
 - general learning in the future, have learning algorithm(s) for all situations  | what must this single algo do
   + interpret rich sensory inputs
   + choose complex actions
  | deep = interpret, reinforcement learning = complex actions
 - brain actually seems to work like deep reinforcement learning (deep learning and reinforcement learning), like basal ganglia = reward function
 - NOW: deep rl great in simple rules, learn simple skills, learn from imitatio
 - CHALLENGING: humans learn much faster, humans can do transfer learning (OPEN PROBLEM), not clearn what reward function should be, not clear what role of prediction should be
 - thought: human main purpose is to sustain life, it may be beneficial for the reward function to be "negative" in the sense that any move that isn't optimal causes neurons in the neural net to be removed at random, this actually mimics real life when neurons are being pared during development. the objective would be to keep as many neurons as possible. if mistakes are made, then the neural net would become smaller but more specialized for the task at hand. if we start with a really large neural net (babies have rly large brains), we won't run into having a neural net that cannot generalize to the problem at hand. if we can couple this with prediction, could be promising 

8/24:
 - Sequential Decision Making
   | state is entire world, underlying; observation is like pixel values
 - Imitation Learning
   | behavior cloning issue with finding new observations, causing slightly
     different action, causing entirely new observations to be made, 
     trajectory more and more off
      + addition of noise and training on distributions with noise added
        alleviates problem, this is very viable and used
      + change distribution of data to closely match distribution of policy, 
        this is done by manipulating the data itself instead of having a 
	perfect policy
 - Objective (in learning autonomously)
   | reward function

   
